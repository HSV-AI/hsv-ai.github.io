<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Huntsville AI  | Word &amp; Document Vectors</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.55.5" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.d98f2eb6bcd1eaedb7edf166bd16af26.css" rel="stylesheet">
    

    

    
      
    

    

    <meta property="og:title" content="Word &amp; Document Vectors" />
<meta property="og:description" content="Word &amp; Document Vectors Huntsville AI - May 8, 2019
Facebook: Huntsville Ai
LinkedIn: Huntsville AI
GitHub: HSV-AI
Mailing List - send an e-mail to jlangley@sessionboard.com to be added
Introduction From Wikipedia:
 Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://hsv-ai.com/meetups/190508_word_document_vectors/" />
<meta property="article:published_time" content="2019-05-08T22:40:19-05:00"/>
<meta property="article:modified_time" content="2019-05-08T22:40:19-05:00"/>

<meta itemprop="name" content="Word &amp; Document Vectors">
<meta itemprop="description" content="Word &amp; Document Vectors Huntsville AI - May 8, 2019
Facebook: Huntsville Ai
LinkedIn: Huntsville AI
GitHub: HSV-AI
Mailing List - send an e-mail to jlangley@sessionboard.com to be added
Introduction From Wikipedia:
 Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension.">


<meta itemprop="datePublished" content="2019-05-08T22:40:19-05:00" />
<meta itemprop="dateModified" content="2019-05-08T22:40:19-05:00" />
<meta itemprop="wordCount" content="1083">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Word &amp; Document Vectors"/>
<meta name="twitter:description" content="Word &amp; Document Vectors Huntsville AI - May 8, 2019
Facebook: Huntsville Ai
LinkedIn: Huntsville AI
GitHub: HSV-AI
Mailing List - send an e-mail to jlangley@sessionboard.com to be added
Introduction From Wikipedia:
 Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension."/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://hsv-ai.com" class="f3 fw2 hover-white no-underline white-90 dib">
      Huntsville AI
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/about/" title="About page">
              About
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/contact/" title="Contact page">
              Contact
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/meetups/" title="Meetups page">
              Meetups
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/projects/" title="Projects page">
              Projects
            </a>
          </li>
          
        </ul>
      
      


<a href="https://www.facebook.com/groups/390465874745286/" target="_blank" class="link-transition facebook link dib z-999 pt3 pt0-l mr1" title="Facebook link" rel="noopener" aria-label="follow on Facebook——Opens in a new window">
  <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>





<a href="https://www.linkedin.com/groups/12177562/" target="_blank" class="link-transition linkedin link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" rel="noopener" aria-label="follow on LinkedIn——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://github.com/HSV-AI" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Github——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>





    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">

    <header class="mt4 w-100">
      <p class="f6 b helvetica tracked">
          
        MEETUPS
      </p>
      <h1 class="f1 athelas mb1">Word &amp; Document Vectors</h1>
      
      <time class="f6 mv4 dib tracked" datetime="2019-05-08T22:40:19-05:00">May 8, 2019</time>      
      
      
    </header>

    <section class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l">

<h1 id="word-document-vectors">Word &amp; Document Vectors</h1>

<p>Huntsville AI - May 8, 2019</p>

<p>Facebook: Huntsville Ai</p>

<p>LinkedIn: Huntsville AI</p>

<p>GitHub: HSV-AI</p>

<p>Mailing List - send an e-mail to jlangley@sessionboard.com to be added</p>

<h1 id="introduction">Introduction</h1>

<p>From <a href="https://en.wikipedia.org/wiki/Word_embedding">Wikipedia</a>:</p>

<blockquote>
<p>Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension.</p>
</blockquote>

<p>The main function is to create a vector of numbers that represent a word based on the context in which that word is used. These vectors can then be used in a relative fashion to determine the relatedness of words.</p>

<h1 id="papers">Papers</h1>

<p>Here is a list of the seminal papers that led to the capability available today for word and document vectors:</p>

<ul>
<li><p><strong>2013</strong> - <em>Distributed Representations of Words and Phrases and their Compositionality</em>
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean -  <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf</a></p></li>

<li><p><strong>2013</strong> - <em>Efficient Estimation of Word Representations in Vector Space</em> Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean - <a href="https://arxiv.org/pdf/1301.3781.pdf">https://arxiv.org/pdf/1301.3781.pdf</a></p></li>

<li><p><strong>2014</strong> - <em>Distributed Representations of Sentences and Documents</em>
Quoc V. Le, Tomas Mikolov - <a href="https://arxiv.org/pdf/1405.4053">https://arxiv.org/pdf/1405.4053</a></p></li>

<li><p><strong>2015</strong> - <em>From Word Embeddings To Document Distances</em> Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, Kilian Q. Weinberger - <a href="http://proceedings.mlr.press/v37/kusnerb15.pdf">http://proceedings.mlr.press/v37/kusnerb15.pdf</a></p></li>
</ul>

<h2 id="word-vectors">Word Vectors</h2>

<p>In order to compute the word vectors, we create a neural network and train it to predict things based on either a Skip-gram or Continuous Bag of Words approach. The weights of the hidden layer then become the values used in the word vector.</p>

<p><img src="https://cdn-images-1.medium.com/max/1000/0*DY41kNV4X5j_PfXA.png" alt="alt text" /></p>

<p>Image from <a href="https://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in
Vector Space</a></p>

<p><img src="https://multithreaded.stitchfix.com/assets/posts/2016-05-27-lda2vec/anim00.gif" alt="Word2Vec" /></p>

<p>Image from <a href="https://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec">StitchFix Blog</a></p>

<h2 id="document-vectors">Document Vectors</h2>

<p>Document Vectors are created by adding an additional document (or paragraph) ID as an input.</p>

<p><img src="https://cdn-images-1.medium.com/max/800/0*x-gtU4UlO8FAsRvL." alt="alt text" /></p>

<p><img src="https://cdn-images-1.medium.com/max/800/0*NtIsrbd4VQzUKVKr." alt="alt text" /></p>

<p>Images from <a href="https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e">A gentle introduction to Doc2Vec
</a></p>

<p><img src="https://www.tensorflow.org/images/linear-relationships.png" alt="Word Vector Image" /></p>

<p>Image from <a href="https://www.tensorflow.org/tutorials/representation/word2vec">Vector Representations of Words</a></p>

<p><img src="https://cdn-images-1.medium.com/max/800/0*XMW5mf81LSHodnTi.png" alt="Cosine Similarity" /></p>

<p>Image from <a href="https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa">Introduction to Word Embedding and Word2Vec</a></p>

<h2 id="word-movers-distance">Word Movers Distance</h2>

<p>The distance between words of different sentences can be used to judge the similarity of the sentences or documents.</p>

<p><img src="https://cdn-images-1.medium.com/max/800/1*nTWAm46JMYWXpHVsS9MA5w.png" alt="Word Movers Distance" /></p>

<p>Image from <a href="https://towardsdatascience.com/word-distance-between-word-embeddings-cc3e9cf1d632">From Word Embeddings To Document Distances</a></p>

<h1 id="references">References</h1>

<p><a href="https://github.com/stanfordnlp/GloVe">https://github.com/stanfordnlp/GloVe</a></p>

<p><a href="https://github.com/fastai/word-embeddings-workshop">https://github.com/fastai/word-embeddings-workshop</a></p>

<p><a href="https://towardsdatascience.com/lda2vec-word-embeddings-in-topic-models-4ee3fc4b2843">https://towardsdatascience.com/lda2vec-word-embeddings-in-topic-models-4ee3fc4b2843</a></p>

<p><a href="https://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec">https://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec</a></p>

<p><a href="https://stackoverflow.com/questions/38287772/cbow-v-s-skip-gram-why-invert-context-and-target-words">https://stackoverflow.com/questions/38287772/cbow-v-s-skip-gram-why-invert-context-and-target-words</a></p>

<h1 id="application">Application</h1>

<p>Below, we will walk through some examples from the Fast.ai Word Embeddings Workshop.</p>

<pre><code class="language-python">#Get the data and untar it

!wget http://files.fast.ai/models/glove_50_glove_100.tgz 

!tar xvzf glove_50_glove_100.tgz
</code></pre>

<pre><code>--2019-05-08 18:34:26--  http://files.fast.ai/models/glove_50_glove_100.tgz
Resolving files.fast.ai (files.fast.ai)... 67.205.15.147
Connecting to files.fast.ai (files.fast.ai)|67.205.15.147|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 225083583 (215M) [text/plain]
Saving to: ‘glove_50_glove_100.tgz.1’

glove_50_glove_100. 100%[===================&gt;] 214.66M   107MB/s    in 2.0s    

2019-05-08 18:34:28 (107 MB/s) - ‘glove_50_glove_100.tgz.1’ saved [225083583/225083583]

glove_vectors_100d.npy
glove_vectors_50d.npy
words.txt
wordsidx.txt
</code></pre>

<pre><code class="language-python">#import packages needed

import pickle
import numpy as np
import re
import json

np.set_printoptions(precision=4, suppress=True)
</code></pre>

<p>We will load the glove 50 &amp; 100 vectors as numpy arrays and load the words and word indices as arrays.</p>

<pre><code class="language-python">vecs = np.load(&quot;glove_vectors_100d.npy&quot;)
vecs50 = np.load(&quot;glove_vectors_50d.npy&quot;)

with open('words.txt') as f:
    content = f.readlines()
words = [x.strip() for x in content]

wordidx = json.load(open('wordsidx.txt'))

</code></pre>

<p>Now let&rsquo;s see what this data looks like&hellip;</p>

<pre><code class="language-python">len(words)
</code></pre>

<pre><code>400000
</code></pre>

<pre><code class="language-python">words[:10]
</code></pre>

<pre><code>['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '&quot;', &quot;'s&quot;]
</code></pre>

<pre><code class="language-python">wordidx['architect']
</code></pre>

<pre><code>4493
</code></pre>

<pre><code class="language-python">words[4493]
</code></pre>

<pre><code>'architect'
</code></pre>

<p>What about that word vector?</p>

<pre><code class="language-python">vecs[4493]
</code></pre>

<pre><code>array([-0.2675, -0.1654, -0.6296,  0.448 ,  0.452 , -0.6424,  0.042 ,
       -1.0137, -0.6596,  0.9818,  0.2998, -0.7044,  0.0481, -0.3909,
       -0.2515, -0.0907,  0.5111,  0.2321, -1.3972,  0.0896, -1.0962,
        0.1159,  0.0979, -0.7837,  0.1524, -1.3648, -0.5557, -1.0818,
       -0.2341, -0.6261, -0.8803,  0.536 ,  0.1439,  0.335 , -0.4361,
       -0.0788,  0.2288, -0.4465,  0.6148, -0.2139,  0.4312,  0.1618,
        1.0763,  0.4359,  0.4286, -0.3155, -0.0784, -0.5784, -0.1905,
        0.1904, -0.1977, -0.5946,  0.6593,  0.2798, -0.0671, -1.6904,
       -0.9657,  0.044 ,  0.3146, -0.491 ,  0.3345,  0.3266,  0.3003,
        0.4409,  0.7353, -0.599 ,  0.1626,  1.012 , -0.3043, -0.1179,
       -0.3546,  0.6402, -0.8409, -0.3581,  0.1925, -1.1535,  0.6362,
        0.8889, -0.0116, -0.2549,  0.3039,  0.2562, -0.0331,  0.4997,
       -0.0159,  0.3529, -0.2008, -0.5076, -0.4175, -1.4415,  0.7295,
       -0.8933,  0.5672,  0.607 ,  0.0374,  0.0441, -0.2491, -1.014 ,
        0.0384, -0.5015], dtype=float32)
</code></pre>

<pre><code class="language-python">from scipy.spatial.distance import cosine as dist
</code></pre>

<p>Smaller numbers mean two words are closer together, larger numbers mean they are further apart.</p>

<p>The distance between similar words is low:</p>

<pre><code class="language-python">dist(vecs[wordidx[&quot;puppy&quot;]], vecs[wordidx[&quot;dog&quot;]])
</code></pre>

<pre><code>0.27636247873306274
</code></pre>

<p>And the distance between unrelated words is high:</p>

<pre><code class="language-python">dist(vecs[wordidx[&quot;avalanche&quot;]], vecs[wordidx[&quot;antique&quot;]])
</code></pre>

<pre><code>0.9621107056736946
</code></pre>

<h1 id="data-bias">Data Bias</h1>

<p>The word vectors will pick up any bias that exists in the data used to build the vectors:</p>

<pre><code class="language-python">dist(vecs[wordidx[&quot;man&quot;]], vecs[wordidx[&quot;genius&quot;]])
</code></pre>

<pre><code>0.5098515152931213
</code></pre>

<pre><code class="language-python">dist(vecs[wordidx[&quot;woman&quot;]], vecs[wordidx[&quot;genius&quot;]])
</code></pre>

<pre><code>0.689783364534378
</code></pre>

<h1 id="nearest-neighbors">Nearest Neighbors</h1>

<p>We can also see what words are close to a given word.</p>

<pre><code class="language-python">from sklearn.neighbors import NearestNeighbors


neigh = NearestNeighbors(n_neighbors=10, radius=0.5, metric='cosine', algorithm='brute')
neigh.fit(vecs)

distances, indices = neigh.kneighbors([vecs[wordidx[&quot;antique&quot;]]])

[(words[int(ind)], dist) for ind, dist in zip(list(indices[0]), list(distances[0]))]
</code></pre>

<pre><code>[('antique', 1.1920929e-07),
 ('antiques', 0.18471009),
 ('furniture', 0.2613591),
 ('jewelry', 0.26212162),
 ('vintage', 0.28011894),
 ('handmade', 0.32542467),
 ('furnishings', 0.3287084),
 ('reproductions', 0.33931458),
 ('decorative', 0.35905504),
 ('pottery', 0.3720798)]
</code></pre>

<h1 id="math-with-word-vectors">Math with Word Vectors</h1>

<p>You can do some pretty interesting things with these word vectors. We can combine multiple terms and use them as a single input.</p>

<pre><code class="language-python">new_vec = vecs[wordidx[&quot;artificial&quot;]] + vecs[wordidx[&quot;intelligence&quot;]]
print(new_vec)
</code></pre>

<pre><code>[ 0.0345 -0.1185  0.746   0.3256  0.3256 -1.4699 -0.8715 -0.9421  0.0679
  0.922   0.6811 -0.3729  1.0969  0.7196  1.3515  1.2493  0.6621  0.1901
 -0.2707 -0.0444 -1.232   0.1744  0.7577 -0.9177 -1.2184  0.6959 -0.1966
 -0.415  -0.3358  0.5452  0.589  -0.0299 -0.9744 -0.8937  0.2283 -0.2092
 -1.3795  1.7811  0.2269  0.47   -0.3045 -0.1573 -0.478   0.3071  0.4202
 -0.4434  0.1602  0.1443 -0.9528 -0.5565  0.7537  0.182   1.4008  1.8967
  0.595  -3.0072  0.6811 -0.2557  2.0217  0.7825  0.4251  1.3615  0.5902
 -0.1312  0.9344 -0.5377 -0.3988 -0.6415  0.6527  0.5117  0.7315  0.1396
  0.3785 -0.6403 -0.094   0.1076  0.6197  0.2537 -1.4346  1.169   1.6931
  0.1458 -0.5981  0.8195 -3.1903  1.2429  2.1481  1.6004  0.2014 -0.2121
  0.3698 -0.001  -0.628   0.2869  0.3119 -0.1093 -0.6341 -1.7804  0.5857
  0.3702]
</code></pre>

<pre><code class="language-python">distances, indices = neigh.kneighbors([new_vec])

[(words[int(ind)], dist) for ind, dist in zip(list(indices[0]), list(distances[0]))]
</code></pre>

<pre><code>[('intelligence', 0.1883161),
 ('artificial', 0.25617576),
 ('information', 0.3256532),
 ('knowledge', 0.336419),
 ('secret', 0.36480355),
 ('human', 0.36726683),
 ('biological', 0.37090683),
 ('using', 0.3773631),
 ('scientific', 0.38513905),
 ('communication', 0.3869152)]
</code></pre>

<p>You can even move from one place to another in the vector space. Beware of bias taking you in unintended directions though. Here&rsquo;s the general sense of the word &ldquo;programmer&rdquo;</p>

<pre><code class="language-python">distances, indices = neigh.kneighbors([vecs[wordidx[&quot;programmer&quot;]]])
[(words[int(ind)], dist) for ind, dist in zip(list(indices[0]), list(distances[0]))]
</code></pre>

<pre><code>[('programmer', 0.0),
 ('programmers', 0.32259798),
 ('animator', 0.36951017),
 ('software', 0.38250887),
 ('computer', 0.40600342),
 ('technician', 0.41406858),
 ('engineer', 0.4303757),
 ('user', 0.4356534),
 ('translator', 0.43721014),
 ('linguist', 0.44948018)]
</code></pre>

<p>Here&rsquo;s the masculine sense of the word &ldquo;programmer&rdquo;</p>

<pre><code class="language-python">new_vec = vecs[wordidx[&quot;programmer&quot;]] + vecs[wordidx[&quot;he&quot;]] - vecs[wordidx[&quot;she&quot;]]
distances, indices = neigh.kneighbors([new_vec])
[(words[int(ind)], dist) for ind, dist in zip(list(indices[0]), list(distances[0]))]
</code></pre>

<pre><code>[('programmer', 0.17419636),
 ('programmers', 0.4133587),
 ('engineer', 0.46376407),
 ('compiler', 0.46731704),
 ('software', 0.4681465),
 ('animator', 0.4892366),
 ('computer', 0.5046158),
 ('mechanic', 0.5150067),
 ('setup', 0.51882535),
 ('developer', 0.51953185)]
</code></pre>

<p>Here&rsquo;s the feminine sense of the word &ldquo;programmer&rdquo;</p>

<pre><code class="language-python">new_vec = vecs[wordidx[&quot;programmer&quot;]] - vecs[wordidx[&quot;he&quot;]] + vecs[wordidx[&quot;she&quot;]]
distances, indices = neigh.kneighbors([new_vec])
[(words[int(ind)], dist) for ind, dist in zip(list(indices[0]), list(distances[0]))]
</code></pre>

<pre><code>[('programmer', 0.19503415),
 ('stylist', 0.42715955),
 ('animator', 0.4820645),
 ('programmers', 0.48337305),
 ('choreographer', 0.4862678),
 ('technician', 0.4862805),
 ('designer', 0.48710012),
 ('prodigy', 0.49118334),
 ('lets', 0.49730027),
 ('screenwriter', 0.49754214)]
</code></pre>
<ul class="pa0">
  
</ul>
<div class="mt6">
        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "hsv-ai" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      </div>
    </section>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://hsv-ai.com" >
    &copy; 2019 Huntsville AI
  </a>
    <div>


<a href="https://www.facebook.com/groups/390465874745286/" target="_blank" class="link-transition facebook link dib z-999 pt3 pt0-l mr1" title="Facebook link" rel="noopener" aria-label="follow on Facebook——Opens in a new window">
  <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>





<a href="https://www.linkedin.com/groups/12177562/" target="_blank" class="link-transition linkedin link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" rel="noopener" aria-label="follow on LinkedIn——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://github.com/HSV-AI" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Github——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>




</div>
  </div>
  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-139852367-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

</footer>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
